{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3216644f-452a-4053-b61c-b493047530f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Problem 2 Interface Compliance\n",
      "==================================================\n",
      "\n",
      "TestEnvironmentInterface:\n",
      "  PASS: test_action_application\n",
      "  PASS: test_environment_initialization\n",
      "  PASS: test_observation_extraction\n",
      "  PASS: test_position_validation\n",
      "  PASS: test_reset_interface\n",
      "  PASS: test_step_interface\n",
      "\n",
      "TestModelInterface:\n",
      "  PASS: test_agent_dqn_forward\n",
      "  PASS: test_agent_dqn_initialization\n",
      "  PASS: test_dueling_dqn_forward\n",
      "  PASS: test_dueling_dqn_initialization\n",
      "\n",
      "TestReplayBufferInterface:\n",
      "  PASS: test_prioritized_buffer_initialization\n",
      "  PASS: test_prioritized_buffer_interfaces\n",
      "  PASS: test_replay_buffer_initialization\n",
      "  PASS: test_replay_buffer_push\n",
      "  PASS: test_replay_buffer_sample\n",
      "\n",
      "==================================================\n",
      "Results: 15/15 tests passed\n",
      "All interface tests passed\n"
     ]
    }
   ],
   "source": [
    "!python ./test_interfaces.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a352a0-67cf-4777-bbad-e7b5e6df1844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n",
      "/Users/robin/Desktop/EE 641/hw4/problem2/./train.py:167: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1728928983675/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  sA_t = torch.tensor([apply_observation_mask(x, self.args.mode) for x in sA], dtype=torch.float32)\n",
      "Episode: 100 | Reward: -0.80 | Success: False\n",
      "[Episode 100] Target networks updated.\n",
      "Episode: 200 | Reward: 81.10 | Success: False\n",
      "[Episode 200] Target networks updated.\n",
      "Episode: 300 | Reward: 71.40 | Success: False\n",
      "[Episode 300] Target networks updated.\n",
      "Episode: 400 | Reward: 12.90 | Success: False\n",
      "[Episode 400] Target networks updated.\n",
      "Episode: 500 | Reward: 93.70 | Success: False\n",
      "[Episode 500] Target networks updated.\n",
      "[Episode 500] Models saved to results/agent_models\n",
      "Episode: 600 | Reward: 83.20 | Success: False\n",
      "[Episode 600] Target networks updated.\n",
      "Episode: 700 | Reward: 19.30 | Success: False\n",
      "[Episode 700] Target networks updated.\n",
      "Episode: 800 | Reward: -5.00 | Success: False\n",
      "[Episode 800] Target networks updated.\n",
      "Episode: 900 | Reward: 85.30 | Success: False\n",
      "[Episode 900] Target networks updated.\n",
      "Episode: 1000 | Reward: 100.00 | Success: False\n",
      "[Episode 1000] Target networks updated.\n",
      "[Episode 1000] Models saved to results/agent_models\n",
      "Episode: 1100 | Reward: 60.10 | Success: False\n",
      "[Episode 1100] Target networks updated.\n",
      "Episode: 1200 | Reward: 83.20 | Success: False\n",
      "[Episode 1200] Target networks updated.\n",
      "Episode: 1300 | Reward: 64.30 | Success: False\n",
      "[Episode 1300] Target networks updated.\n",
      "Episode: 1400 | Reward: 91.60 | Success: False\n",
      "[Episode 1400] Target networks updated.\n",
      "Episode: 1500 | Reward: 81.10 | Success: False\n",
      "[Episode 1500] Target networks updated.\n",
      "[Episode 1500] Models saved to results/agent_models\n",
      "Episode: 1600 | Reward: 89.50 | Success: False\n",
      "[Episode 1600] Target networks updated.\n",
      "Episode: 1700 | Reward: 72.70 | Success: False\n",
      "[Episode 1700] Target networks updated.\n",
      "Episode: 1800 | Reward: 83.20 | Success: False\n",
      "[Episode 1800] Target networks updated.\n",
      "Episode: 1900 | Reward: 83.20 | Success: False\n",
      "[Episode 1900] Target networks updated.\n",
      "Episode: 2000 | Reward: -5.00 | Success: False\n",
      "[Episode 2000] Target networks updated.\n",
      "[Episode 2000] Models saved to results/agent_models\n",
      "Episode: 2100 | Reward: 43.30 | Success: False\n",
      "[Episode 2100] Target networks updated.\n",
      "Episode: 2200 | Reward: 97.90 | Success: False\n",
      "[Episode 2200] Target networks updated.\n",
      "Episode: 2300 | Reward: -5.00 | Success: False\n",
      "[Episode 2300] Target networks updated.\n",
      "Episode: 2400 | Reward: 89.50 | Success: False\n",
      "[Episode 2400] Target networks updated.\n",
      "Episode: 2500 | Reward: 87.40 | Success: False\n",
      "[Episode 2500] Target networks updated.\n",
      "[Episode 2500] Models saved to results/agent_models\n",
      "Episode: 2600 | Reward: 1.30 | Success: False\n",
      "[Episode 2600] Target networks updated.\n",
      "Episode: 2700 | Reward: 68.50 | Success: False\n",
      "[Episode 2700] Target networks updated.\n",
      "Episode: 2800 | Reward: 76.90 | Success: False\n",
      "[Episode 2800] Target networks updated.\n",
      "Episode: 2900 | Reward: 68.50 | Success: False\n",
      "[Episode 2900] Target networks updated.\n",
      "Episode: 3000 | Reward: 93.70 | Success: False\n",
      "[Episode 3000] Target networks updated.\n",
      "[Episode 3000] Models saved to results/agent_models\n",
      "Episode: 3100 | Reward: 91.60 | Success: False\n",
      "[Episode 3100] Target networks updated.\n",
      "Episode: 3200 | Reward: 39.10 | Success: False\n",
      "[Episode 3200] Target networks updated.\n",
      "Episode: 3300 | Reward: 85.30 | Success: False\n",
      "[Episode 3300] Target networks updated.\n",
      "Episode: 3400 | Reward: 93.70 | Success: False\n",
      "[Episode 3400] Target networks updated.\n",
      "Episode: 3500 | Reward: 95.80 | Success: False\n",
      "[Episode 3500] Target networks updated.\n",
      "[Episode 3500] Models saved to results/agent_models\n",
      "Episode: 3600 | Reward: 81.10 | Success: False\n",
      "[Episode 3600] Target networks updated.\n",
      "Episode: 3700 | Reward: 74.80 | Success: False\n",
      "[Episode 3700] Target networks updated.\n",
      "Episode: 3800 | Reward: 49.60 | Success: False\n",
      "[Episode 3800] Target networks updated.\n",
      "Episode: 3900 | Reward: 93.70 | Success: False\n",
      "[Episode 3900] Target networks updated.\n",
      "Episode: 4000 | Reward: 87.40 | Success: False\n",
      "[Episode 4000] Target networks updated.\n",
      "[Episode 4000] Models saved to results/agent_models\n",
      "Episode: 4100 | Reward: 45.40 | Success: False\n",
      "[Episode 4100] Target networks updated.\n",
      "Episode: 4200 | Reward: -5.00 | Success: False\n",
      "[Episode 4200] Target networks updated.\n",
      "Episode: 4300 | Reward: 89.50 | Success: False\n",
      "[Episode 4300] Target networks updated.\n",
      "Episode: 4400 | Reward: 60.10 | Success: False\n",
      "[Episode 4400] Target networks updated.\n",
      "Episode: 4500 | Reward: -5.00 | Success: False\n",
      "[Episode 4500] Target networks updated.\n",
      "[Episode 4500] Models saved to results/agent_models\n",
      "Episode: 4600 | Reward: 83.20 | Success: False\n",
      "[Episode 4600] Target networks updated.\n",
      "Episode: 4700 | Reward: 81.10 | Success: False\n",
      "[Episode 4700] Target networks updated.\n",
      "Episode: 4800 | Reward: 87.40 | Success: False\n",
      "[Episode 4800] Target networks updated.\n",
      "Episode: 4900 | Reward: 76.90 | Success: False\n",
      "[Episode 4900] Target networks updated.\n",
      "Episode: 5000 | Reward: 43.30 | Success: False\n",
      "[Episode 5000] Target networks updated.\n",
      "[Episode 5000] Models saved to results/agent_models\n",
      "Training complete. Final models saved to results/agent_models\n",
      "Evaluation: mean_reward=24.19, success_rate=0.00\n",
      "Namespace(grid_size=[10, 10], max_steps=50, num_episodes=5000, batch_size=32, lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995, hidden_dim=64, target_update=100, mode='full', seed=641, save_freq=500)\n"
     ]
    }
   ],
   "source": [
    "!python ./train.py --mode=full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd71822-cd66-495d-8742-97547a91c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/robin/Desktop/EE 641/hw4/problem2/./evaluate.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_A.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"agent_A.pt\"), map_location=\"cpu\"))\n",
      "/Users/robin/Desktop/EE 641/hw4/problem2/./evaluate.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_B.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"agent_B.pt\"), map_location=\"cpu\"))\n",
      "Evaluation complete.\n",
      "{\n",
      "    \"performance\": {\n",
      "        \"mean_reward\": -3.5929999999999995,\n",
      "        \"median_reward\": -4.999999999999998,\n",
      "        \"success_rate\": 0.0,\n",
      "        \"avg_steps\": 50.0,\n",
      "        \"episodes\": 100\n",
      "    },\n",
      "    \"communication\": {\n",
      "        \"A_mean\": 0.7991800308227539,\n",
      "        \"A_std\": 0.0711529552936554,\n",
      "        \"A_max\": 0.9647209048271179,\n",
      "        \"A_min\": 0.5727786421775818,\n",
      "        \"B_mean\": 0.4580570161342621,\n",
      "        \"B_std\": 0.1833084523677826,\n",
      "        \"B_max\": 0.9590024352073669,\n",
      "        \"B_min\": 0.157365083694458,\n",
      "        \"correlation\": -0.6973408697533818\n",
      "    },\n",
      "    \"generalization\": {\n",
      "        \"mean_reward\": -4.789999999999999,\n",
      "        \"success_rate\": 0.0,\n",
      "        \"configs_tested\": 10\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python ./evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83703618-4e7a-4dde-b8c6-22735d79c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n",
      "/Users/robin/Desktop/EE 641/hw4/problem2/./train.py:167: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1728928983675/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  sA_t = torch.tensor([apply_observation_mask(x, self.args.mode) for x in sA], dtype=torch.float32)\n",
      "Episode: 100 | Reward: 13.90 | Success: False\n",
      "[Episode 100] Target networks updated.\n",
      "Episode: 200 | Reward: 72.70 | Success: False\n",
      "[Episode 200] Target networks updated.\n",
      "Episode: 300 | Reward: -5.00 | Success: False\n",
      "[Episode 300] Target networks updated.\n",
      "Episode: 400 | Reward: 79.00 | Success: False\n",
      "[Episode 400] Target networks updated.\n",
      "Episode: 500 | Reward: -5.00 | Success: False\n",
      "[Episode 500] Target networks updated.\n",
      "[Episode 500] Models saved to results/agent_models\n",
      "Episode: 600 | Reward: 87.40 | Success: False\n",
      "[Episode 600] Target networks updated.\n",
      "Episode: 700 | Reward: -5.00 | Success: False\n",
      "[Episode 700] Target networks updated.\n",
      "Episode: 800 | Reward: 41.20 | Success: False\n",
      "[Episode 800] Target networks updated.\n",
      "Episode: 900 | Reward: 95.80 | Success: False\n",
      "[Episode 900] Target networks updated.\n",
      "Episode: 1000 | Reward: -5.00 | Success: False\n",
      "[Episode 1000] Target networks updated.\n",
      "[Episode 1000] Models saved to results/agent_models\n",
      "Episode: 1100 | Reward: 87.40 | Success: False\n",
      "[Episode 1100] Target networks updated.\n",
      "Episode: 1200 | Reward: -5.00 | Success: False\n",
      "[Episode 1200] Target networks updated.\n",
      "Episode: 1300 | Reward: 87.40 | Success: False\n",
      "[Episode 1300] Target networks updated.\n",
      "Episode: 1400 | Reward: -5.00 | Success: False\n",
      "[Episode 1400] Target networks updated.\n",
      "Episode: 1500 | Reward: 72.70 | Success: False\n",
      "[Episode 1500] Target networks updated.\n",
      "[Episode 1500] Models saved to results/agent_models\n",
      "Episode: 1600 | Reward: 9.80 | Success: False\n",
      "[Episode 1600] Target networks updated.\n",
      "Episode: 1700 | Reward: -5.00 | Success: False\n",
      "[Episode 1700] Target networks updated.\n",
      "Episode: 1800 | Reward: 49.60 | Success: False\n",
      "[Episode 1800] Target networks updated.\n",
      "Episode: 1900 | Reward: 85.30 | Success: False\n",
      "[Episode 1900] Target networks updated.\n",
      "Episode: 2000 | Reward: -5.00 | Success: False\n",
      "[Episode 2000] Target networks updated.\n",
      "[Episode 2000] Models saved to results/agent_models\n",
      "Episode: 2100 | Reward: -5.00 | Success: False\n",
      "[Episode 2100] Target networks updated.\n",
      "Episode: 2200 | Reward: 58.00 | Success: False\n",
      "[Episode 2200] Target networks updated.\n",
      "Episode: 2300 | Reward: -5.00 | Success: False\n",
      "[Episode 2300] Target networks updated.\n",
      "Episode: 2400 | Reward: 85.30 | Success: False\n",
      "[Episode 2400] Target networks updated.\n",
      "Episode: 2500 | Reward: -5.00 | Success: False\n",
      "[Episode 2500] Target networks updated.\n",
      "[Episode 2500] Models saved to results/agent_models\n",
      "Episode: 2600 | Reward: -5.00 | Success: False\n",
      "[Episode 2600] Target networks updated.\n",
      "Episode: 2700 | Reward: 95.80 | Success: False\n",
      "[Episode 2700] Target networks updated.\n",
      "Episode: 2800 | Reward: 79.00 | Success: False\n",
      "[Episode 2800] Target networks updated.\n",
      "Episode: 2900 | Reward: 91.60 | Success: False\n",
      "[Episode 2900] Target networks updated.\n",
      "Episode: 3000 | Reward: 18.00 | Success: False\n",
      "[Episode 3000] Target networks updated.\n",
      "[Episode 3000] Models saved to results/agent_models\n",
      "Episode: 3100 | Reward: -5.00 | Success: False\n",
      "[Episode 3100] Target networks updated.\n",
      "Episode: 3200 | Reward: -5.00 | Success: False\n",
      "[Episode 3200] Target networks updated.\n",
      "Episode: 3300 | Reward: -5.00 | Success: False\n",
      "[Episode 3300] Target networks updated.\n",
      "Episode: 3400 | Reward: -5.00 | Success: False\n",
      "[Episode 3400] Target networks updated.\n",
      "Episode: 3500 | Reward: 26.50 | Success: False\n",
      "[Episode 3500] Target networks updated.\n",
      "[Episode 3500] Models saved to results/agent_models\n",
      "Episode: 3600 | Reward: 76.90 | Success: False\n",
      "[Episode 3600] Target networks updated.\n",
      "Episode: 3700 | Reward: 83.20 | Success: False\n",
      "[Episode 3700] Target networks updated.\n",
      "Episode: 3800 | Reward: 76.90 | Success: False\n",
      "[Episode 3800] Target networks updated.\n",
      "Episode: 3900 | Reward: -5.00 | Success: False\n",
      "[Episode 3900] Target networks updated.\n",
      "Episode: 4000 | Reward: -5.00 | Success: False\n",
      "[Episode 4000] Target networks updated.\n",
      "[Episode 4000] Models saved to results/agent_models\n",
      "Episode: 4100 | Reward: -5.00 | Success: False\n",
      "[Episode 4100] Target networks updated.\n",
      "Episode: 4200 | Reward: 72.70 | Success: False\n",
      "[Episode 4200] Target networks updated.\n",
      "Episode: 4300 | Reward: 22.00 | Success: False\n",
      "[Episode 4300] Target networks updated.\n",
      "Episode: 4400 | Reward: 91.60 | Success: False\n",
      "[Episode 4400] Target networks updated.\n",
      "Episode: 4500 | Reward: 81.10 | Success: False\n",
      "[Episode 4500] Target networks updated.\n",
      "[Episode 4500] Models saved to results/agent_models\n",
      "Episode: 4600 | Reward: 75.90 | Success: False\n",
      "[Episode 4600] Target networks updated.\n",
      "Episode: 4700 | Reward: -5.00 | Success: False\n",
      "[Episode 4700] Target networks updated.\n",
      "Episode: 4800 | Reward: -5.00 | Success: False\n",
      "[Episode 4800] Target networks updated.\n",
      "Episode: 4900 | Reward: -5.00 | Success: False\n",
      "[Episode 4900] Target networks updated.\n",
      "Episode: 5000 | Reward: -5.00 | Success: False\n",
      "[Episode 5000] Target networks updated.\n",
      "[Episode 5000] Models saved to results/agent_models\n",
      "Training complete. Final models saved to results/agent_models\n",
      "Evaluation: mean_reward=22.30, success_rate=0.00\n",
      "Namespace(grid_size=[10, 10], max_steps=50, num_episodes=5000, batch_size=32, lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995, hidden_dim=64, target_update=100, mode='independent', seed=641, save_freq=500)\n"
     ]
    }
   ],
   "source": [
    "!python ./train.py --mode=independent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a86f929-1ff0-4185-9cf7-ea61bf7fb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/robin/Desktop/EE 641/hw4/problem2/./evaluate.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_A.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"agent_A.pt\"), map_location=\"cpu\"))\n",
      "/Users/robin/Desktop/EE 641/hw4/problem2/./evaluate.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_B.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"agent_B.pt\"), map_location=\"cpu\"))\n",
      "Evaluation complete.\n",
      "{\n",
      "    \"performance\": {\n",
      "        \"mean_reward\": 0.08200000000000049,\n",
      "        \"median_reward\": -4.999999999999998,\n",
      "        \"success_rate\": 0.0,\n",
      "        \"avg_steps\": 50.0,\n",
      "        \"episodes\": 100\n",
      "    },\n",
      "    \"communication\": {\n",
      "        \"A_mean\": 0.8404520153999329,\n",
      "        \"A_std\": 0.07788722962141037,\n",
      "        \"A_max\": 0.9751209616661072,\n",
      "        \"A_min\": 0.6726166605949402,\n",
      "        \"B_mean\": 0.2273109257221222,\n",
      "        \"B_std\": 0.09912950545549393,\n",
      "        \"B_max\": 0.47735416889190674,\n",
      "        \"B_min\": 0.1266120821237564,\n",
      "        \"correlation\": -0.1542173936288572\n",
      "    },\n",
      "    \"generalization\": {\n",
      "        \"mean_reward\": -4.999999999999998,\n",
      "        \"success_rate\": 0.0,\n",
      "        \"configs_tested\": 10\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python ./evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6857d95e-1cdd-42f6-a78d-9e8de76cb416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n",
      "/Users/robin/Desktop/EE 641/hw4/problem2/./train.py:167: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1728928983675/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  sA_t = torch.tensor([apply_observation_mask(x, self.args.mode) for x in sA], dtype=torch.float32)\n",
      "Episode: 100 | Reward: -5.00 | Success: False\n",
      "[Episode 100] Target networks updated.\n",
      "Episode: 200 | Reward: -5.00 | Success: False\n",
      "[Episode 200] Target networks updated.\n",
      "Episode: 300 | Reward: 64.30 | Success: False\n",
      "[Episode 300] Target networks updated.\n",
      "Episode: 400 | Reward: -5.00 | Success: False\n",
      "[Episode 400] Target networks updated.\n",
      "Episode: 500 | Reward: -5.00 | Success: False\n",
      "[Episode 500] Target networks updated.\n",
      "[Episode 500] Models saved to results/agent_models\n",
      "Episode: 600 | Reward: -5.00 | Success: False\n",
      "[Episode 600] Target networks updated.\n",
      "Episode: 700 | Reward: -5.00 | Success: False\n",
      "[Episode 700] Target networks updated.\n",
      "Episode: 800 | Reward: 95.80 | Success: False\n",
      "[Episode 800] Target networks updated.\n",
      "Episode: 900 | Reward: -5.00 | Success: False\n",
      "[Episode 900] Target networks updated.\n",
      "Episode: 1000 | Reward: 68.50 | Success: False\n",
      "[Episode 1000] Target networks updated.\n",
      "[Episode 1000] Models saved to results/agent_models\n",
      "Episode: 1100 | Reward: 66.40 | Success: False\n",
      "[Episode 1100] Target networks updated.\n",
      "Episode: 1200 | Reward: 85.30 | Success: False\n",
      "[Episode 1200] Target networks updated.\n",
      "Episode: 1300 | Reward: 87.40 | Success: False\n",
      "[Episode 1300] Target networks updated.\n",
      "Episode: 1400 | Reward: 97.90 | Success: False\n",
      "[Episode 1400] Target networks updated.\n",
      "Episode: 1500 | Reward: 74.80 | Success: False\n",
      "[Episode 1500] Target networks updated.\n",
      "[Episode 1500] Models saved to results/agent_models\n",
      "Episode: 1600 | Reward: 87.40 | Success: False\n",
      "[Episode 1600] Target networks updated.\n",
      "Episode: 1700 | Reward: 70.60 | Success: False\n",
      "[Episode 1700] Target networks updated.\n",
      "Episode: 1800 | Reward: 60.10 | Success: False\n",
      "[Episode 1800] Target networks updated.\n",
      "Episode: 1900 | Reward: -5.00 | Success: False\n",
      "[Episode 1900] Target networks updated.\n",
      "Episode: 2000 | Reward: 76.90 | Success: False\n",
      "[Episode 2000] Target networks updated.\n",
      "[Episode 2000] Models saved to results/agent_models\n",
      "Episode: 2100 | Reward: 53.80 | Success: False\n",
      "[Episode 2100] Target networks updated.\n",
      "Episode: 2200 | Reward: -5.00 | Success: False\n",
      "[Episode 2200] Target networks updated.\n",
      "Episode: 2300 | Reward: 64.30 | Success: False\n",
      "[Episode 2300] Target networks updated.\n",
      "Episode: 2400 | Reward: -5.00 | Success: False\n",
      "[Episode 2400] Target networks updated.\n",
      "Episode: 2500 | Reward: 60.10 | Success: False\n",
      "[Episode 2500] Target networks updated.\n",
      "[Episode 2500] Models saved to results/agent_models\n",
      "Episode: 2600 | Reward: 39.10 | Success: False\n",
      "[Episode 2600] Target networks updated.\n",
      "Episode: 2700 | Reward: 79.00 | Success: False\n",
      "[Episode 2700] Target networks updated.\n",
      "Episode: 2800 | Reward: -5.00 | Success: False\n",
      "[Episode 2800] Target networks updated.\n",
      "Episode: 2900 | Reward: 12.50 | Success: False\n",
      "[Episode 2900] Target networks updated.\n",
      "Episode: 3000 | Reward: 91.60 | Success: False\n",
      "[Episode 3000] Target networks updated.\n",
      "[Episode 3000] Models saved to results/agent_models\n",
      "Episode: 3100 | Reward: 3.40 | Success: False\n",
      "[Episode 3100] Target networks updated.\n",
      "Episode: 3200 | Reward: 49.60 | Success: False\n",
      "[Episode 3200] Target networks updated.\n",
      "Episode: 3300 | Reward: 70.60 | Success: False\n",
      "[Episode 3300] Target networks updated.\n",
      "Episode: 3400 | Reward: 79.00 | Success: False\n",
      "[Episode 3400] Target networks updated.\n",
      "Episode: 3500 | Reward: -5.00 | Success: False\n",
      "[Episode 3500] Target networks updated.\n",
      "[Episode 3500] Models saved to results/agent_models\n",
      "Episode: 3600 | Reward: 85.30 | Success: False\n",
      "[Episode 3600] Target networks updated.\n",
      "Episode: 3700 | Reward: 83.20 | Success: False\n",
      "[Episode 3700] Target networks updated.\n",
      "Episode: 3800 | Reward: 9.70 | Success: False\n",
      "[Episode 3800] Target networks updated.\n",
      "Episode: 3900 | Reward: 89.50 | Success: False\n",
      "[Episode 3900] Target networks updated.\n",
      "Episode: 4000 | Reward: 87.40 | Success: False\n",
      "[Episode 4000] Target networks updated.\n",
      "[Episode 4000] Models saved to results/agent_models\n",
      "Episode: 4100 | Reward: -5.00 | Success: False\n",
      "[Episode 4100] Target networks updated.\n",
      "Episode: 4200 | Reward: -5.00 | Success: False\n",
      "[Episode 4200] Target networks updated.\n",
      "Episode: 4300 | Reward: -5.00 | Success: False\n",
      "[Episode 4300] Target networks updated.\n",
      "Episode: 4400 | Reward: -5.00 | Success: False\n",
      "[Episode 4400] Target networks updated.\n",
      "Episode: 4500 | Reward: -5.00 | Success: False\n",
      "[Episode 4500] Target networks updated.\n",
      "[Episode 4500] Models saved to results/agent_models\n",
      "Episode: 4600 | Reward: -5.00 | Success: False\n",
      "[Episode 4600] Target networks updated.\n",
      "Episode: 4700 | Reward: 87.40 | Success: False\n",
      "[Episode 4700] Target networks updated.\n",
      "Episode: 4800 | Reward: -5.00 | Success: False\n",
      "[Episode 4800] Target networks updated.\n",
      "Episode: 4900 | Reward: -5.00 | Success: False\n",
      "[Episode 4900] Target networks updated.\n",
      "Episode: 5000 | Reward: -5.00 | Success: False\n",
      "[Episode 5000] Target networks updated.\n",
      "[Episode 5000] Models saved to results/agent_models\n",
      "Training complete. Final models saved to results/agent_models\n",
      "Evaluation: mean_reward=40.36, success_rate=0.00\n",
      "Namespace(grid_size=[10, 10], max_steps=50, num_episodes=5000, batch_size=32, lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.995, hidden_dim=64, target_update=100, mode='comm', seed=641, save_freq=500)\n"
     ]
    }
   ],
   "source": [
    "!python ./train.py --mode=comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1ac1a8-fab2-4868-a463-601fae272ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/robin/Desktop/EE 641/hw4/problem2/./evaluate.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_A.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"agent_A.pt\"), map_location=\"cpu\"))\n",
      "/Users/robin/Desktop/EE 641/hw4/problem2/./evaluate.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_B.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"agent_B.pt\"), map_location=\"cpu\"))\n",
      "Evaluation complete.\n",
      "{\n",
      "    \"performance\": {\n",
      "        \"mean_reward\": -1.85,\n",
      "        \"median_reward\": -4.999999999999998,\n",
      "        \"success_rate\": 0.0,\n",
      "        \"avg_steps\": 50.0,\n",
      "        \"episodes\": 100\n",
      "    },\n",
      "    \"communication\": {\n",
      "        \"A_mean\": 0.7583188414573669,\n",
      "        \"A_std\": 0.1747807413339615,\n",
      "        \"A_max\": 0.9769450426101685,\n",
      "        \"A_min\": 0.3026348650455475,\n",
      "        \"B_mean\": 0.730617105960846,\n",
      "        \"B_std\": 0.17834025621414185,\n",
      "        \"B_max\": 0.9794703125953674,\n",
      "        \"B_min\": 0.09088767319917679,\n",
      "        \"correlation\": 0.3023275727646967\n",
      "    },\n",
      "    \"generalization\": {\n",
      "        \"mean_reward\": -2.6899999999999986,\n",
      "        \"success_rate\": 0.0,\n",
      "        \"configs_tested\": 10\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!python ./evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6688ed5e-01b8-409e-84f6-d891626739c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
